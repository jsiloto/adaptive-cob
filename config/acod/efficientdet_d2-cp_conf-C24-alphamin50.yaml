dataset:
    name: &dataset_name 'coco2017'
    root: &root_dir !join ['./resource/dataset/', *dataset_name]
    num_workers: 4
    aspect_ratio_group_factor: 3
    splits:
        train:
            images: !join [*root_dir, '/train2017']
            annotations: !join [*root_dir, '/annotations/instances_train2017.json']
            remove_non_annotated_imgs: True
            jpeg_quality:
        val:
            images: !join [*root_dir, '/val2017']
            annotations: !join [*root_dir, '/annotations/instances_val2017.json']
            remove_non_annotated_imgs: True
            jpeg_quality:
        test:
            images: !join [*root_dir, '/val2017']
            annotations: !join [*root_dir, '/annotations/instances_val2017.json']
            remove_non_annotated_imgs: True
            jpeg_quality:

teacher_model:
    name: &teacher_model_name 'efficientdet'
    backbone:
        name: &teacher_backbone_name 'efficientdet'
        params:
            pretrained: True
            compound_coef:  2
            freeze_layers: True
            weights: './resource/ckpt/efficientdet-d2.pth'
    params:
        num_classes: 91
        pretrained: True
    experiment: &teacher_experiment !join [*dataset_name, '-', *teacher_model_name, '-backbone_', *teacher_backbone_name]
    ckpt: !join ['./resource/ckpt/org/', *teacher_experiment, '.pt']

student_model:
    name: &student_model_name 'efficientdet'
    backbone:
        name: &student_backbone_name 'efficientdet'
        params:
            pretrained: True
            compound_coef: &coef 2
            freeze_layers: False
            fully_slimmable: True
            slimmable: True
            weights: './resource/ckpt/efficientdet-d2.pth'
            width_copies: 4
            width_mult_list: &wdl [0.5, 0.58, 0.66, 0.75, 0.83, 0.916, 1.0 ]
            bottleneck:
                name: 'MBConvBlockEfficientDet'
                bottleneck_channel: &bch 24

    bottleneck_transformer:
        order: ['quantizer', 'dequantizer']
        components:
            quantizer:
                params:
                    num_bits: 8
            dequantizer:
                params:
                    num_bits: 8
    params:
        num_classes: 91
        pretrained: True
    distill_backbone_only: True
    unfrozen_modules: ['backbone_net.model._conv_stem',
                       'backbone_net.model._bn0',
                       'backbone_net.model._blocks.0',
                       'backbone_net.model._blocks.1',
                       'backbone_net.model._blocks.2',
                       'backbone_net.model._blocks.3',
                       'backbone_net.model._blocks.4',
                       'backbone_net.model._blocks.5',
                       'backbone_net.model._blocks.6',
                       'backbone_net.model._blocks.7',
                       'backbone_net.bottleneck']
    experiment: &student_experiment !join [*dataset_name, '-', *student_model_name, '-backbone_', *student_backbone_name, '-d', *coef, '-cp_conf', '-C', *bch, '-alphamin50']
    ckpt: !join ['./resource/ckpt/acod/', *student_experiment, '.pt']

train:
    num_epochs: 12
    batch_size: 8
    log_freq: 1000
    optimizer:
        type: 'Adam'
        params:
            lr: 0.0005
    criterion:
        type: 'general'
        params:
            org_loss_factor: 0.0
        terms:
            bottleneck:
                ts_modules: ['backbone_net.bottleneck', 'backbone_net.bottleneck']
                criterion:
                    type: 'MSELoss'
                    params:
                        reduction: 'sum'
                factor: 1.0
            p4:
                ts_modules: [ 'p4', 'p4' ]
                criterion:
                    type: 'MSELoss'
                    params:
                        reduction: 'sum'
                factor: 1.0
            p5:
                ts_modules: [ 'p5', 'p5' ]
                criterion:
                    type: 'MSELoss'
                    params:
                        reduction: 'sum'
                factor: 1.0

    scheduler:
        type: 'MultiStepLR'
        params:
            milestones: [2, 4, 6, 8]
            gamma: 0.5


test:
    batch_size: 1
